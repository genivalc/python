{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06\n",
    "# Adicionando funções externas a API da Gemini\n",
    "\n",
    "Um grande salto de possibilidades de utilizações únicas da LLMs ocorrreu quando a OpenAI lançou o function calling. Essa ferramenta permite adicionarmos manualmente funções externas ao modelo que ele, dependendo da situação, poderá utilizar para obter novas informações ou atuar em diversos escopos. Vamos fazer uma breve revisão de como utilizamos funções externas na api da OpenAI, este assunto é explorado mais afundo no curso de Explorando a API da OpenAI. Na próxima aula, mostraremos como o framework langChain facilita a utilização das funções externas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importações iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from google import genai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "client = genai.Client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando função que será adicionada ao modelo\n",
    "\n",
    "Utilizaremos uma função simples que simula uma api de tempo, que retorna a temperatura de um determinado local. Lembrando que modelos de llm são treinados com dados históricos, portanto, não possuem informações atuais. A única forma de eles entenderem o que está ocorrendo neste instante é passando informações pra eles através de prompts ou de funções externas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local, unidade=\"celsius\"):\n",
    "    if \"são paulo\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"São Paulo\", \"temperatura\": \"32\", \"unidade\": unidade}\n",
    "            )\n",
    "    elif \"porto alegre\" in local.lower():\n",
    "        return json.dumps(\n",
    "            {\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": unidade}\n",
    "            )\n",
    "    else:\n",
    "        return json.dumps(\n",
    "            {\"local\": local, \"temperatura\": \"unknown\"}\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"local\": \"Porto Alegre\", \"temperatura\": \"25\", \"unidade\": \"celsius\"}'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obter_temperatura_atual('Porto Alegre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando descrição da função\n",
    "\n",
    "Através dessa descrição o modelo entenderá o que a função faz e como ela pode ser utilizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_temperatura_atual(local: str, unidade: str = \"celsius\") -> str:\n",
    "    \"\"\"\n",
    "    Obtém a temperatura atual em uma dada cidade.\n",
    "\n",
    "    Args:\n",
    "        local: O nome da cidade. Ex: São Paulo\n",
    "        unidade: Unidade de temperatura desejada (\"celsius\" ou \"fahrenheit\"). Padrão é celsius.\n",
    "\n",
    "    Returns:\n",
    "        Uma string representando a temperatura atual.\n",
    "    \"\"\"\n",
    "    # Aqui você faria a chamada a uma API de clima real, como OpenWeatherMap\n",
    "    if unidade == \"celsius\":\n",
    "        return f\"A temperatura atual em {local} é 25°C\"\n",
    "    elif unidade == \"fahrenheit\":\n",
    "        return f\"A temperatura atual em {local} é 77°F\"\n",
    "    else:\n",
    "        return \"Unidade inválida\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chamando o modelo com a nova ferramenta\n",
    "\n",
    "Para chamar o modelo com a ferramenta criada, basta passar o argumento tools com uma lista de ferramentas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(\n",
       "  automatic_function_calling_history=[],\n",
       "  candidates=[\n",
       "    Candidate(\n",
       "      avg_logprobs=-0.21319308545854357,\n",
       "      content=Content(\n",
       "        parts=[\n",
       "          Part(\n",
       "            text=\"\"\"No momento, em Porto Alegre, a temperatura é de 25°C. A sensação térmica é de 26°C. O tempo está parcialmente nublado.\n",
       "\"\"\"\n",
       "          ),\n",
       "        ],\n",
       "        role='model'\n",
       "      ),\n",
       "      finish_reason=<FinishReason.STOP: 'STOP'>\n",
       "    ),\n",
       "  ],\n",
       "  model_version='gemini-2.0-flash-001',\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=11>\n",
       "  ),\n",
       "  usage_metadata=GenerateContentResponseUsageMetadata(\n",
       "    candidates_token_count=36,\n",
       "    candidates_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=36\n",
       "      ),\n",
       "    ],\n",
       "    prompt_token_count=8,\n",
       "    prompt_tokens_details=[\n",
       "      ModalityTokenCount(\n",
       "        modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "        token_count=8\n",
       "      ),\n",
       "    ],\n",
       "    total_token_count=44\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "mensagens = types.Content(\n",
    "    role='user', \n",
    "    parts=[{\"text\": \"Qual é temperatura em Porto Alegre agora?\"}]\n",
    ")\n",
    "\n",
    "# Para Gemini, utilize generate_content\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001', contents=mensagens\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando a resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos perceber que o conteúdo da resposta veio vazio, pois para a pergunta \"Qual é a temperatura em Porto Alegre?\" ele necessitará chamar a função antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No momento, em Porto Alegre, a temperatura é de 25°C. A sensação térmica é de 26°C. O tempo está parcialmente nublado.\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagem = response.candidates[0].content.parts[0].text\n",
    "mensagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No momento, em Porto Alegre, a temperatura é de 25°C. A sensação térmica é de 26°C. O tempo está parcialmente nublado.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mensagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chamando novamente o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explorando diferentes perguntas e o parâmetro tool_choice\n",
    "\n",
    "Através do parâmetro tool_choice é possível forçar o modelo a sempre utilizar uma tool. Vamos ver como ele se comporta para diferentes perguntas modificando o parâmetro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetro \"auto\"\n",
    "\n",
    "Assim o modelo define automaticamente se é necessária a utilização de uma função ou não"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BaseModel.__init__() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m      3\u001b[39m mensagens = types.Content(\n\u001b[32m      4\u001b[39m     role=\u001b[33m'\u001b[39m\u001b[33muser\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m      5\u001b[39m     parts=[{\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\"\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m \u001b[33m\"\"\"\u001b[39m}]\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Para Gemini, utilize generate_content\u001b[39;00m\n\u001b[32m     23\u001b[39m response = client.models.generate_content(\n\u001b[32m     24\u001b[39m     model=\u001b[33m'\u001b[39m\u001b[33mgemini-2.0-flash-001\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     25\u001b[39m     contents=mensagens,\n\u001b[32m     26\u001b[39m     config=types.GenerateContentConfig(tools=[obter_temperatura_atual]),\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     function= \u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFunctionDeclaration\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobter_temperatura_atual\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m )\n\u001b[32m     29\u001b[39m response\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Para Gemini, a resposta está em response.candidates[0].content.parts[0].text\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: BaseModel.__init__() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "mensagens = types.Content(\n",
    "    role='user', \n",
    "    parts=[{\"text\": \"\"\"\n",
    "Dado o pedido do usuário, se for necessário consultar a temperatura, \n",
    "responda SOMENTE com um JSON no formato:\n",
    "{\n",
    "  \"function_call\": {\n",
    "    \"name\": \"obter_temperatura_atual\",\n",
    "    \"args\": {\n",
    "      \"local\": \"São Paulo\",\n",
    "      \"unidade\": \"celsius\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "Usuário: Qual a temperatura em São Paulo?\n",
    "\"\"\"}]\n",
    ")\n",
    "\n",
    "# Para Gemini, utilize generate_content\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001', \n",
    "    contents=mensagens,\n",
    "    config=types.GenerateContentConfig(tools=[obter_temperatura_atual]),\n",
    ")\n",
    "response\n",
    "\n",
    "# Para Gemini, a resposta está em response.candidates[0].content.parts[0].text\n",
    "mensagem = response.candidates[0].content.parts[0].text\n",
    "print('Conteúdo:', mensagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parâmetro \"function\"\n",
    "\n",
    "Podemos fazer o modelo rodar obrigatoriamente a função, passando dentro de um dicionário a função que o modelo deve rodar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=None args={'location': 'Boston, MA'} name='get_current_weather'\n",
      "Conteúdo: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from google.genai import types\n",
    "\n",
    "function = types.FunctionDeclaration(\n",
    "    name='get_current_weather',\n",
    "    description='Get the current weather in a given location',\n",
    "    parameters_json_schema={\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'location': {\n",
    "                'type': 'string',\n",
    "                'description': 'The city and state, e.g. San Francisco, CA',\n",
    "            }\n",
    "        },\n",
    "        'required': ['location'],\n",
    "    },\n",
    ")\n",
    "\n",
    "tool = types.Tool(function_declarations=[function])\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents='What is the weather like in Boston?',\n",
    "    config=types.GenerateContentConfig(tools=[tool]),\n",
    ")\n",
    "\n",
    "print(response.function_calls[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
