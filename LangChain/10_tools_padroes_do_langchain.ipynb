{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools padrão do LangChain\n",
    "\n",
    "A biblioteca LangChain já possui diversas ferramentas padrão construídas que podemos utilizar. Elas podem ser verificadas neste link: https://python.langchain.com/docs/integrations/tools/.\n",
    "Vamos mostrar como utilizar algumas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ArXiv\n",
    "\n",
    "Esta ferramenta utiliza a biblioteca do arXiv para retornar resumos de artigos científicos de um determinado tema solicitado.\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/arxiv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temos três formas em geral para chamar uma tool, da mais baixo a mais alto nível:\n",
    "- Criando a tool utilziando o Wrapper: no LangChain foram criados diferentes wrappers para apis e bibliotecas externas que facilitam a utilização das mesmas e permitem uma rápida criação de uma nova tool. Ela é mais customizável, pois podemos modificar as descrições e argumentos da tool às nossa necessidades, além de podermos modificar facilmente os parâmetros do wrapper\n",
    "- Instanciando a tool já criada: em geral, uma tool pronta já está criada dentro da biblioteca e podemos acessá-la diretamente\n",
    "- Utilizando o load_tools: o LangChain possui uma ferramnta especial chamada load_tools que permite o carregamento de diversas ferramentas ao mesmo tempo\n",
    "\n",
    "Vamos explorar os três métodos agora:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando tool manualmente através do Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.arxiv import ArxivAPIWrapper\n",
    "from pydantic import BaseModel, Field #Importação atualizada\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "class ArxivArgs(BaseModel):\n",
    "    query:str = Field(description='Qyuery de busca no ArXiv')\n",
    "\n",
    "tool_arxiv = StructuredTool.from_function(\n",
    "    func=ArxivAPIWrapper(top_k_results=3).run,\n",
    "    args_schema=ArxivArgs,\n",
    "    name='arxiv',\n",
    "    description = (\n",
    "        \"Uma ferramenta em torno do Arxiv.org. \"\n",
    "        \"Útil para quando você precisa responder a perguntas sobre Física, Matemática, \"\n",
    "        \"Ciência da Computação, Biologia Quantitativa, Finanças Quantitativas, Estatística, \"\n",
    "        \"Engenharia Elétrica e Economia utilizando artigos científicos do arxiv.org. \"\n",
    "        \"A entrada deve ser uma consulta de pesquisa em inglês.\"\n",
    "    ),\n",
    "    return_direct=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: Uma ferramenta em torno do Arxiv.org. Útil para quando você precisa responder a perguntas sobre Física, Matemática, Ciência da Computação, Biologia Quantitativa, Finanças Quantitativas, Estatística, Engenharia Elétrica e Economia utilizando artigos científicos do arxiv.org. A entrada deve ser uma consulta de pesquisa em inglês.\n",
      "Args: {'query': {'description': 'Qyuery de busca no ArXiv', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-12-23\\nTitle: Trustworthy and Efficient LLMs Meet Databases\\nAuthors: Kyoungmin Kim, Anastasia Ailamaki\\nSummary: In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.\\n\\nPublished: 2024-06-13\\nTitle: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\\nAuthors: Irene Weber\\nSummary: Large Language Models (LLMs) have become widely adopted recently. Research\\nexplores their use both as autonomous agents and as tools for software\\nengineering. LLM-integrated applications, on the other hand, are software\\nsystems that leverage an LLM to perform tasks that would otherwise be\\nimpossible or require significant coding effort. While LLM-integrated\\napplication engineering is emerging as new discipline, its terminology,\\nconcepts and methods need to be established. This study provides a taxonomy for\\nLLM-integrated applications, offering a framework for analyzing and describing\\nthese systems. It also demonstrates various ways to utilize LLMs in\\napplications, as well as options for implementing such integrations.\\n  Following established methods, we analyze a sample of recent LLM-integrated\\napplications to identify relevant dimensions. We evaluate the taxonomy by\\napplying it to additional cases. This review shows that applications integrate\\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\\nLLM integrations, which we term ``LLM components''. To gain a clear\\nunderstanding of an application's architecture, we examine each LLM component\\nseparately. We identify thirteen dimensions along which to characterize an LLM\\ncomponent, including the LLM skills leveraged, the format of the output, and\\nmore. LLM-integrated applications are described as combinations of their LLM\\ncomponents. We suggest a concise representation using feature vectors for\\nvisualization.\\n  The taxonomy is effective for describing LLM-integrated applications. It can\\ncontribute to theory building in the nascent field of LLM-integrated\\napplication engineering and aid in developing such systems. Researchers and\\npractitioners explore numerous creative ways to leverage LLMs in applications.\\nThough challenges persist, integrating LLMs may revolutionize the way software\\nsystems are built.\\n\\nPublished: 2024-05-30\\nTitle: Parrot: Efficient Serving of LLM-based Applications with Semantic Variable\\nAuthors: Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu\\nSummary: The rise of large language models (LLMs) has enabled LLM-based applications\\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\\nstrength of LLM and conventional software. Diverse LLM applications from\\ndifferent tenants could design complex workflows using multiple LLM requests to\\naccomplish one task. However, they have to use the over-simplified\\nrequest-level API provided by today's public LLM services, losing essential\\napplication-level information. Public LLM services have to blindly optimize\\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\\napplications.\\n  This paper introduces Parrot, an LLM service system that focuses on the\\nend-to-end experience of \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv.run({'query': 'llm'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instanciando tool já criada no LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "\n",
    "tool_arxiv = ArxivQueryRun(api_wrapper=ArxivAPIWrapper(top_k_results=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "Args: {'query': {'description': 'search query to look up', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando tool através do load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = load_tools(['arxiv'])\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=3, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=4000))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv = tools[0]\n",
    "tool_arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
      "Args: {'query': {'description': 'search query to look up', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print('Descrição:', tool_arxiv.description)\n",
    "print('Args:', tool_arxiv.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Published: 2024-12-23\\nTitle: Trustworthy and Efficient LLMs Meet Databases\\nAuthors: Kyoungmin Kim, Anastasia Ailamaki\\nSummary: In the rapidly evolving AI era with large language models (LLMs) at the core,\\nmaking LLMs more trustworthy and efficient, especially in output generation\\n(inference), has gained significant attention. This is to reduce plausible but\\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\\ninference demands. This tutorial explores such efforts and makes them\\ntransparent to the database community. Understanding these efforts is essential\\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\\nnew opportunities and challenges in their intersection. This tutorial aims to\\nshare with database researchers and practitioners essential concepts and\\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\\nin the intersection between LLMs and databases.\\n\\nPublished: 2024-06-13\\nTitle: Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications\\nAuthors: Irene Weber\\nSummary: Large Language Models (LLMs) have become widely adopted recently. Research\\nexplores their use both as autonomous agents and as tools for software\\nengineering. LLM-integrated applications, on the other hand, are software\\nsystems that leverage an LLM to perform tasks that would otherwise be\\nimpossible or require significant coding effort. While LLM-integrated\\napplication engineering is emerging as new discipline, its terminology,\\nconcepts and methods need to be established. This study provides a taxonomy for\\nLLM-integrated applications, offering a framework for analyzing and describing\\nthese systems. It also demonstrates various ways to utilize LLMs in\\napplications, as well as options for implementing such integrations.\\n  Following established methods, we analyze a sample of recent LLM-integrated\\napplications to identify relevant dimensions. We evaluate the taxonomy by\\napplying it to additional cases. This review shows that applications integrate\\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\\nLLM integrations, which we term ``LLM components''. To gain a clear\\nunderstanding of an application's architecture, we examine each LLM component\\nseparately. We identify thirteen dimensions along which to characterize an LLM\\ncomponent, including the LLM skills leveraged, the format of the output, and\\nmore. LLM-integrated applications are described as combinations of their LLM\\ncomponents. We suggest a concise representation using feature vectors for\\nvisualization.\\n  The taxonomy is effective for describing LLM-integrated applications. It can\\ncontribute to theory building in the nascent field of LLM-integrated\\napplication engineering and aid in developing such systems. Researchers and\\npractitioners explore numerous creative ways to leverage LLMs in applications.\\nThough challenges persist, integrating LLMs may revolutionize the way software\\nsystems are built.\\n\\nPublished: 2024-05-30\\nTitle: Parrot: Efficient Serving of LLM-based Applications with Semantic Variable\\nAuthors: Chaofan Lin, Zhenhua Han, Chengruidong Zhang, Yuqing Yang, Fan Yang, Chen Chen, Lili Qiu\\nSummary: The rise of large language models (LLMs) has enabled LLM-based applications\\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\\nstrength of LLM and conventional software. Diverse LLM applications from\\ndifferent tenants could design complex workflows using multiple LLM requests to\\naccomplish one task. However, they have to use the over-simplified\\nrequest-level API provided by today's public LLM services, losing essential\\napplication-level information. Public LLM services have to blindly optimize\\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\\napplications.\\n  This paper introduces Parrot, an LLM service system that focuses on the\\nend-to-end experience of \""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv.run({'query': 'llm'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python REPL\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A Python shell. Use this to execute python commands. Input should be a valid python command. If you want to see the output of a value, you should print it out with `print(...)`.\n",
      "Args: {'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_experimental.tools.python.tool import PythonREPLTool\n",
    "\n",
    "tool_repl = PythonREPLTool()\n",
    "print('Descrição:', tool_repl.description)\n",
    "print('Args:', tool_repl.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oi\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_repl.run({'query': 'print(\"oi\")'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StackOverflow\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/stackexchange/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrição: A wrapper around StackExchange. Useful for when you need to answer specific programming questionscode excerpts, code examples and solutionsInput should be a fully formed question.\n",
      "Args: {'query': {'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "tools = load_tools(['stackexchange'])\n",
    "tool_stack = tools[0]\n",
    "print('Descrição:', tool_stack.description)\n",
    "print('Args:', tool_stack.args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: LangGraph deployment with docker-compose\\nsslmode=disable\\n\\nWhen I run docker-compose up, I get the following error:\\nFileNotFoundError: [Errno 2] No such file or directory: &#39;/deps/__outer_src/src/<span class=\"highlight\">agent</span>\\\\\\\\agent.py&#39;\\nCould not find <span class=\"highlight\">python</span> file for &hellip; /src/<span class=\"highlight\">agent</span>/agent.py file? I tried moving <span class=\"highlight\">Python</span> source from the src to root folder, but it didn&#39;t help. There&#39;s very little info about what langgraph CLI tool does and how. &hellip; \\n\\nQuestion: Tool Methods in Python Class Missing self Argument When Used in Custom Agents\\nI am working on a custom implementation of AI <span class=\"highlight\">agents</span> using <span class=\"highlight\">Python</span>, where the <span class=\"highlight\">agents</span> rely on <span class=\"highlight\">langchain</span> tools (methods) for interacting with a PostgreSQL database. &hellip; These tools are later used by CrewAI <span class=\"highlight\">agents</span> to perform various tasks. &hellip; \\n\\nQuestion: Error: &#39;&gt;=&#39; not supported between instances of &#39;int&#39; and &#39;str&#39; when searching for jobs using LangChain and OpenAI\\nI&#39;m working on a <span class=\"highlight\">Python</span> script that uses <span class=\"highlight\">LangChain</span> and OpenAI to search for machine learning jobs based on my CV. &hellip; Environment:\\n<span class=\"highlight\">Python</span> 3.x\\nLibraries: <span class=\"highlight\">langchain</span>-openai, browser-use, <span class=\"highlight\">python</span>-dotenv, PyPDF2\\nOpenAI model: gpt-4\\nQuestion:\\nWhy is the &gt;= operator being used internally, and how can I resolve this type mismatch &hellip; '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_stack.run({'query': 'LangChain Agents Python'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File System\n",
    "\n",
    "https://python.langchain.com/v0.1/docs/integrations/tools/filesystem/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: copy_file\n",
      "Descrição: Create a copy of a file in a specified location\n",
      "Args: {'source_path': {'description': 'Path of the file to copy', 'title': 'Source Path', 'type': 'string'}, 'destination_path': {'description': 'Path to save the copied file', 'title': 'Destination Path', 'type': 'string'}}\n",
      "\n",
      "Name: file_delete\n",
      "Descrição: Delete a file\n",
      "Args: {'file_path': {'description': 'Path of the file to delete', 'title': 'File Path', 'type': 'string'}}\n",
      "\n",
      "Name: file_search\n",
      "Descrição: Recursively search for files in a subdirectory that match the regex pattern\n",
      "Args: {'dir_path': {'default': '.', 'description': 'Subdirectory to search in.', 'title': 'Dir Path', 'type': 'string'}, 'pattern': {'description': 'Unix shell regex, where * matches everything.', 'title': 'Pattern', 'type': 'string'}}\n",
      "\n",
      "Name: move_file\n",
      "Descrição: Move or rename a file from one location to another\n",
      "Args: {'source_path': {'description': 'Path of the file to move', 'title': 'Source Path', 'type': 'string'}, 'destination_path': {'description': 'New path for the moved file', 'title': 'Destination Path', 'type': 'string'}}\n",
      "\n",
      "Name: read_file\n",
      "Descrição: Read file from disk\n",
      "Args: {'file_path': {'description': 'name of file', 'title': 'File Path', 'type': 'string'}}\n",
      "\n",
      "Name: write_file\n",
      "Descrição: Write file to disk\n",
      "Args: {'file_path': {'description': 'name of file', 'title': 'File Path', 'type': 'string'}, 'text': {'description': 'text to write to file', 'title': 'Text', 'type': 'string'}, 'append': {'default': False, 'description': 'Whether to append to an existing file.', 'title': 'Append', 'type': 'boolean'}}\n",
      "\n",
      "Name: list_directory\n",
      "Descrição: List files and directories in a specified folder\n",
      "Args: {'dir_path': {'default': '.', 'description': 'Subdirectory to list.', 'title': 'Dir Path', 'type': 'string'}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits.file_management.toolkit import FileManagementToolkit\n",
    "\n",
    "tool_kit = FileManagementToolkit(\n",
    "    root_dir='arquivos'\n",
    ")\n",
    "tools = tool_kit.get_tools()\n",
    "for tool in tools:\n",
    "    print('Name:', tool.name)\n",
    "    print('Descrição:', tool.description)\n",
    "    print('Args:', tool.args) \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dando um exemplo de aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.file_management.toolkit import FileManagementToolkit\n",
    "\n",
    "tool_kit = FileManagementToolkit(\n",
    "    root_dir='arquivos',\n",
    "    selected_tools=['write_file', 'read_file', 'file_search','list_directory']\n",
    ")\n",
    "tools = tool_kit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "tools_json = [convert_to_openai_function(tool) for tool in tools]\n",
    "tool_run = {tool.name: tool for tool in tools}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Você é um assistente amigável chamado Isaac capaz de gerenciar arquivos'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "chat = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.agents import AgentFinish #Importação do AgentFinish atualizada\n",
    "\n",
    "def roteamento(resultado):\n",
    "    if isinstance(resultado, AgentFinish):\n",
    "        return resultado.return_values['output']\n",
    "    else:\n",
    "        return tool_run[resultado.tool].run(resultado.tool_input)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "\n",
    "\n",
    "chain = prompt | chat.bind(functions=tools_json) | OpenAIFunctionsAgentOutputParser() | roteamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu sou um modelo de linguagem grande, treinado pelo Google. Eu posso escrever e executar código.\n",
      "Eu posso procurar por arquivos pdf, mas preciso de um diretório para procurar. Você poderia me dizer qual diretório você gostaria que eu procurasse?\n",
      "Eu posso procurar arquivos txt para você. Em qual pasta você gostaria que eu procurasse?\n",
      "File written successfully to notas.txt.\n",
      "File written successfully to notas.txt.\n",
      "Isso foi feito por uma LLM\n"
     ]
    }
   ],
   "source": [
    "# === IMPORTAÇÕES NECESSÁRIAS ===\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain_core.agents import AgentFinish\n",
    "\n",
    "# === TOOL:  ===\n",
    "\n",
    "from langchain_community.agent_toolkits.file_management.toolkit import FileManagementToolkit\n",
    "\n",
    "tool_kit = FileManagementToolkit(\n",
    "    root_dir='arquivos',\n",
    "    selected_tools=['write_file', 'read_file', 'file_search','list_directory']\n",
    ")\n",
    "tools = tool_kit.get_tools()\n",
    "\n",
    "# === DEFININDO TOOLS ===\n",
    "# tools = [tool_kit.get_tools()]\n",
    "tools_json = [convert_to_openai_function(tool) for tool in tools]\n",
    "tool_run = {tool.name: tool for tool in tools}\n",
    "\n",
    "\n",
    "# === MODELO ===\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-lite\")  # ou gemini-1.5-pro, gemini-1.5-flash\n",
    "\n",
    "# === PROMPT ===\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Pense com cuidado ao categorizar o texto conforme as instruções'),\n",
    "    ('user', '{input}')\n",
    "])\n",
    "\n",
    "\n",
    "# === AGENTE COMBINADO COM TOOLS ===\n",
    "chain = (\n",
    "    prompt\n",
    "    | model.bind(functions=tools_json)\n",
    "    | OpenAIFunctionsAgentOutputParser()\n",
    ")\n",
    "\n",
    "# === GERENCIADOR DE EXECUÇÃO ===\n",
    "def roteamento(resultado):\n",
    "    if isinstance(resultado, AgentFinish):\n",
    "        return resultado.return_values['output']\n",
    "    else:\n",
    "        return tool_run[resultado.tool].invoke(resultado.tool_input)\n",
    "\n",
    "\n",
    "# === CADEIA FINAL ===\n",
    "final_chain = chain | roteamento\n",
    "\n",
    "\n",
    "# === EXEMPLO DE USO ===\n",
    "resposta = final_chain.invoke({'input': 'O que você é capz de fazer?'})\n",
    "print(resposta)\n",
    "\n",
    "resposta2 = final_chain.invoke({'input': 'Quais arquivos pdf você tem na pasta?'})\n",
    "print(resposta2)\n",
    "\n",
    "resposta3 = final_chain.invoke({'input': 'Quais arquivos txt você tem na pasta?'})\n",
    "print(resposta3)\n",
    "\n",
    "resposta4 = final_chain.invoke({'input': 'Crie um arquivo txt chamado notas com o seguinte conteúdo \"Isso foi feito por uma LLM\"'})\n",
    "print(resposta4)\n",
    "\n",
    "resposta5 = final_chain.invoke({'input': 'Crie um arquivo txt chamado notas com o seguinte conteúdo \"Isso foi feito por uma LLM\"'})\n",
    "print(resposta5)\n",
    "\n",
    "resposta6 = final_chain.invoke({'input': 'Leia o arquivo notas.txt'})\n",
    "print(resposta6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
